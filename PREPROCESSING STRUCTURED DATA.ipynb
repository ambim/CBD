{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert pandas categorical data for scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "import pandas as pd\n",
    "\n",
    "#create data frame\n",
    "raw_data = {'patient': [1, 1, 1, 2, 2],\n",
    "        'obs': [1, 2, 3, 1, 2],\n",
    "        'treatment': [0, 1, 0, 1, 0],\n",
    "        'score': ['strong', 'weak', 'normal', 'weak', 'strong']}\n",
    "df = pd.DataFrame(raw_data, columns = ['patient', 'obs', 'treatment', 'score'])\n",
    "\n",
    "#label encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "#fit the encoder to the pandas column\n",
    "le.fit(df['score'])\n",
    "\n",
    "#list the labels \n",
    "list(le.classes_)\n",
    "\n",
    "#transform categories into integers\n",
    "le.transform(df['score'])\n",
    "\n",
    "#convert some integers into their categorical names\n",
    "list (le.inverse_transform([2, 2, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting observations with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.2</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0        1.1       11.1\n",
       "1        2.2       22.2\n",
       "2        3.3       33.3\n",
       "3        4.4       44.4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "#Create feature matrix \n",
    "X = np.array([[1.1, 11.1], \n",
    "              [2.2, 22.2], \n",
    "              [3.3, 33.3], \n",
    "              [4.4, 44.4], \n",
    "              [np.nan, 55]])\n",
    "\n",
    "#deleting observations with missing values\n",
    "X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "# Load data as a data frame\n",
    "df = pd.DataFrame(X, columns=['feature_1', 'feature_2'])\n",
    "\n",
    "# Remove observations with missing values\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting outliers\n",
    "EllipticEnvelope assumes the data is normally distributed and based on that assumption “draws” an ellipse around the data, classifying any observation inside the ellipse as an inlier (labeled as 1) and any observation outside the ellipse as an outlier (labeled as -1). A major limitation of this approach is the need to specify a contamination parameter which is the proportion of observations that are outliers, a value that we don’t know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "#Create simulated data\n",
    "X, _ = make_blobs(n_samples = 10, n_features = 2, centers = 1, random_state = 1)\n",
    "\n",
    "#replace the first observation's values with extreme values \n",
    "X[0,0] = 10000\n",
    "X[0,1] = 10000\n",
    "\n",
    "#Crate the detector \n",
    "outlier_detector = EllipticEnvelope(contamination = 0.1)\n",
    "\n",
    "#Fit detector \n",
    "outlier_detector.fit(X)\n",
    "\n",
    "#Predict outliers\n",
    "outlier_detector.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "import numpy as np\n",
    "\n",
    "# Create feature\n",
    "age = np.array([[6], \n",
    "                [12], \n",
    "                [20], \n",
    "                [36], \n",
    "                [65]])\n",
    "\n",
    "#Option1: Binarize feature\n",
    "binarizer = Binarizer(18)\n",
    "binarizer.fit_transform(age)\n",
    "\n",
    "#Option 2: Break into bins\n",
    "np.digitize (age, bins=[20,30,40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Ordinal Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medium</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Medium</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>High</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Score  Scale\n",
       "0     Low      1\n",
       "1     Low      1\n",
       "2  Medium      2\n",
       "3  Medium      2\n",
       "4    High      3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create features\n",
    "df = pd.DataFrame({'Score': ['Low', \n",
    "                             'Low', \n",
    "                             'Medium', \n",
    "                             'Medium', \n",
    "                             'High']})\n",
    "\n",
    "# View data frame\n",
    "df\n",
    "\n",
    "#create a scale map\n",
    "scale_mapper = {'Low':1, 'Medium':2, 'High':3}\n",
    "\n",
    "#Map feature values to scale \n",
    "df['Scale'] = df['Score'].replace(scale_mapper)\n",
    "# View data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Classes with Downsampling & Upsampling\n",
    "In downsampling, we randomly sample without replacement from the majority class (i.e. the class with more observations) to create a new subset of observation equal in size to the minority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# Create feature matrix\n",
    "X = iris.data\n",
    "\n",
    "# Create target vector\n",
    "y = iris.target\n",
    "\n",
    "#Make Iris imbalanced by removing first 40 observations\n",
    "X = X[40:,:]\n",
    "y = y[40:]\n",
    "\n",
    "#Create binary target vector indicating if class 0 \n",
    "y = np.where((y==0), 0, 1)\n",
    "\n",
    "#Look at the imbalanced target vector\n",
    "y \n",
    "\n",
    "#Downsample the majority class to match the minority class \n",
    "#Indices of each class' observations\n",
    "i_class0 = np.where(y == 0)[0]\n",
    "i_class1 = np.where(y == 1)[0] \n",
    "\n",
    "#number of observations in each class \n",
    "n_class0 = len(i_class0) \n",
    "n_class1 = len(i_class1) \n",
    "\n",
    "# For every observation of class 0, randomly sample from class 1 without replacement\n",
    "i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False)\n",
    "\n",
    "# Join together class 0's target vector with the downsampled class 1's target vector\n",
    "np.hstack((y[i_class0], y[i_class1_downsampled]))\n",
    "\n",
    "##UPSAMPLING\n",
    "\n",
    "'''\n",
    "In upsampling, for every observation in the majority class, we randomly select an\n",
    "observation from the minority class with replacement. The end result is the same number \n",
    "of observations from the minority and majority classes.\n",
    "\n",
    "'''\n",
    "\n",
    "# For every observation in class 1, randomly sample from class 0 with replacement\n",
    "i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True)\n",
    "\n",
    "# Join together class 0's upsampled target vector with class 1's target vector\n",
    "np.concatenate((y[i_class0_upsampled], y[i_class1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling outliers: drop, mark or re-scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "      <th>Outlier</th>\n",
       "      <th>Log_of_Square_feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.778956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Bathrooms  Square_Feet  Outlier  Log_of_Square_feet\n",
       "0   534433        2.0         1500        0            7.313220\n",
       "1   392333        3.5         2500        0            7.824046\n",
       "2   293222        2.0         1500        0            7.313220\n",
       "3  4322032      116.0        48000        1           10.778956"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "houses = pd.DataFrame()\n",
    "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
    "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
    "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
    "\n",
    "#OPTION 1: Drop \n",
    "# Drop observations greater than some value\n",
    "houses[houses['Bathrooms'] < 20]\n",
    "\n",
    "#OPTION 2: Mark \n",
    "\n",
    "import numpy as np \n",
    "#create feature based on boolean condition\n",
    "houses['Outlier'] = np.where(houses['Bathrooms'] < 20, 0, 1)\n",
    "\n",
    "#Show data\n",
    "houses\n",
    "\n",
    "#Option 3: rescale \n",
    "#log feature \n",
    "houses['Log_of_Square_feet'] = [np.log(x) for x in houses ['Square_Feet']]\n",
    "houses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing values with means\n",
    "\n",
    "Mean imputation replaces missing values with the mean value of that feature/variable. Mean imputation is one of the most ‘naive’ imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3051    ,  0.49273333],\n",
       "       [ 0.4949    ,  0.2654    ],\n",
       "       [ 0.6974    ,  0.2615    ],\n",
       "       [ 0.3769    ,  0.5846    ],\n",
       "       [ 0.2231    ,  0.4615    ],\n",
       "       [ 0.341     ,  0.8308    ],\n",
       "       [ 0.4436    ,  0.4962    ],\n",
       "       [ 0.5897    ,  0.3269    ],\n",
       "       [ 0.6308    ,  0.5346    ],\n",
       "       [ 0.5       ,  0.6731    ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import Imputer \n",
    "\n",
    "#Create datafrfame \n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Create two variables called x0 and x1. Make the first value of x1 a missing value\n",
    "df['x0'] = [0.3051,0.4949,0.6974,0.3769,0.2231,0.341,0.4436,0.5897,0.6308,0.5]\n",
    "df['x1'] = [np.nan,0.2654,0.2615,0.5846,0.4615,0.8308,0.4962,0.3269,0.5346,0.6731]\n",
    "\n",
    "# View the dataset\n",
    "df\n",
    "\n",
    "#create an imputer object that looks for 'NaN' values, \n",
    "#then replaces them with the mean value of the feature by columns (axis=0)\n",
    "mean_imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis=0)\n",
    "\n",
    "#Train the imputor on the df dataset\n",
    "mean_imputer = mean_imputer.fit(df)\n",
    "\n",
    "#Apply the imputer\n",
    "imputed_df = mean_imputer.transform(df.values)\n",
    "\n",
    "imputed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing class labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 0.  , -0.21, -1.19],\n",
       "       [ 0.  ,  0.87,  1.31],\n",
       "       [ 0.  , -0.67, -0.22]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create feature matrix with categorical feature\n",
    "X = np.array([[0, 2.10, 1.45], \n",
    "              [1, 1.18, 1.33], \n",
    "              [0, 1.22, 1.27],\n",
    "              [0, -0.21, -1.19],\n",
    "              [np.nan, 0.87, 1.31],\n",
    "              [np.nan, -0.67, -0.22]])\n",
    "\n",
    "\n",
    "#Create Imputer Object \n",
    "imputer = Imputer(strategy='most_frequent', axis=0)\n",
    "\n",
    "#Fill missing values with most frequent class\n",
    "# Fill missing values with most frequent class\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing class labels with k-nearest neighbours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 1.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create feature matrix with categorical feature\n",
    "X = np.array([[0, 2.10, 1.45], \n",
    "              [1, 1.18, 1.33], \n",
    "              [0, 1.22, 1.27],\n",
    "              [1, -0.21, -1.19]])\n",
    "\n",
    "# Create feature matrix with missing values in the categorical feature\n",
    "X_with_nan = np.array([[np.nan, 0.87, 1.31], \n",
    "                       [np.nan, -0.67, -0.22]])\n",
    "\n",
    "# Train KNN learner\n",
    "clf = KNeighborsClassifier(3, weights='distance')\n",
    "trained_model = clf.fit(X[:,1:], X[:,0])\n",
    "\n",
    "# Predict missing values' class\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:])\n",
    "\n",
    "# Join column of predicted class with their other features\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))\n",
    "\n",
    "# Join two feature matrices\n",
    "np.vstack((X_with_imputed, X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize observations \n",
    "Normalizer rescales the values on individual observations to have unit norm (the sum of their lengths is one).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678,  0.70710678],\n",
       "       [ 0.30782029,  0.95144452],\n",
       "       [ 0.07405353,  0.99725427],\n",
       "       [ 0.04733062,  0.99887928],\n",
       "       [ 0.95709822,  0.28976368]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np \n",
    "\n",
    "# Create feature matrix\n",
    "X = np.array([[0.5, 0.5], \n",
    "              [1.1, 3.4], \n",
    "              [1.5, 20.2], \n",
    "              [1.63, 34.4], \n",
    "              [10.9, 3.3]])\n",
    "\n",
    "#create normalizer object\n",
    "normalizer = Normalizer(norm = 'l2')\n",
    "\n",
    "#Transform feature matrix \n",
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode features with multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'California', 'Delware', 'Florida', 'Texas'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "# Create NumPy array\n",
    "y = [('Texas', 'Florida'), \n",
    "    ('California', 'Alabama'), \n",
    "    ('Texas', 'Florida'), \n",
    "    ('Delware', 'Florida'), \n",
    "    ('Texas', 'Alabama')]\n",
    "\n",
    "# Create MultiLabelBinarizer object\n",
    "one_hot = MultiLabelBinarizer()\n",
    "\n",
    "# One-hot encode data\n",
    "one_hot.fit_transform(y)\n",
    "\n",
    "#View classes\n",
    "one_hot.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode nominal categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>California</th>\n",
       "      <th>Delaware</th>\n",
       "      <th>Texas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   California  Delaware  Texas\n",
       "0           0         0      1\n",
       "1           1         0      0\n",
       "2           0         0      1\n",
       "3           0         1      0\n",
       "4           0         0      1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Create NumPy array\n",
    "x = np.array([['Texas'], \n",
    "              ['California'], \n",
    "              ['Texas'], \n",
    "              ['Delaware'], \n",
    "              ['Texas']])\n",
    "\n",
    "#create label binarizer object\n",
    "one_hot = LabelBinarizer()\n",
    "\n",
    "#one-hot encode data \n",
    "one_hot.fit_transform(x)\n",
    "\n",
    "#method 2: get dummies \n",
    "\n",
    "# Dummy feature\n",
    "pd.get_dummies(x[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3100623 , -0.49582097,  0.48403749, -0.05143998],\n",
       "       [-0.17225683,  1.92563026, -1.26851205, -1.26670948],\n",
       "       [ 2.23933883, -0.98011121,  1.76924049,  1.43388941],\n",
       "       [ 0.18948252, -0.25367584,  0.36720086,  0.35364985],\n",
       "       [ 1.15412078, -0.49582097,  0.54245581,  0.21861991]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the iris data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create a variable for the feature data\n",
    "X = iris.data\n",
    "\n",
    "# Create a variable for the target data\n",
    "y = iris.target\n",
    "\n",
    "#split the dataset for cross-validation\n",
    "# Random split the data into four new datasets, training features, training outcome, test features, \n",
    "# and test outcome. Set the size of the test data to be 30% of the full dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Load the standard scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation based on the training data\n",
    "sc.fit(X_train)\n",
    "\n",
    "# Scale the training data to be of mean 0 and of unit variance\n",
    "X_train_std = sc.transform(X_train)\n",
    "\n",
    "# Scale the test data to be of mean 0 and of unit variance\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Feature Test Data, non-standardized\n",
    "X_test[0:5]\n",
    "\n",
    "# Feature Test Data, standardized.\n",
    "X_test_std[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescaling a feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.28571429],\n",
       "       [ 0.35714286],\n",
       "       [ 0.42857143],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature\n",
    "x = np.array([[-500.5], \n",
    "              [-100.1], \n",
    "              [0], \n",
    "              [100.1], \n",
    "              [900.9]])\n",
    "\n",
    "# Create scaler\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale feature\n",
    "x_scale = minmax_scale.fit_transform(x)\n",
    "\n",
    "# Show feature\n",
    "x_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
